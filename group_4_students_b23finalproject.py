# -*- coding: utf-8 -*-
"""Group 4_Students_B23FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pialEhmS4IG6yotfVdIs-yLNu7c_5vDP

### Group Members
* Khady Gaye
* Abigail Naa Amankwaa Abeo
* Elie Mulamba
* Regis Konan Marcel Djaha
"""

import numpy as np

import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets

# Get Data: Do not touch it.
def get_data():
  data_url = "http://lib.stat.cmu.edu/datasets/boston"
  raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
  X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
  y = raw_df.values[1::2, 2]
  return X,y

# cgs
def cgs(A):
  """
    Q,R = cgs(A)
    Apply classical Gram-Schmidt to mxn rectangular/square matrix. 

    Parameters
    -------
    A: mxn rectangular/square matrix   

    Returns
    -------
    Q: mxn square matrix
    R: nxn upper triangular matrix

  """
  # ADD YOUR CODES
  m = A.shape[0]  # get the number of rows of A
  n = A.shape[1] # get the number of columns of A

  R = np.zeros((n,n)) # create a zero matrix of nxn
  Q = np.ones((m,n)) # copy A (deep copy)


  for k in range(0,n):
    w = A[:,k]
    #print(w)
    
    for j in range(k-1):
      R[j,k] = Q[:,j].T@w
    
    for j in range(k-1):
      w = w - R[j,k]*Q[:,j]
    
    #norm = np.linalg.norm(w)
    
    R[k,k] = np.linalg.norm(w)
    Q[:,k] = w/R[k,k]
    #print(w/R[k,k])

  return  Q, R

# Implement BACK SUBS
def backsubs(U, b):

  """
  x = backsubs(U, b)
  Apply back substitution for the square upper triangular system Ux=b. 

  Parameters
  -------
    U: nxn square upper triangular array
    b: n array
    

  Returns
  -------
    x: n array
  """

  n= U.shape[1]
  x= np.zeros((n,))
  b_copy= np.copy(b)

  if U[n-1,n-1]==0.0:
    if b[n-1] != 0.0:
      print("System has no solution.")
  
  else:
    x[n-1]= b_copy[n-1]/U[n-1,n-1]
  for i in range(n-2,-1,-1):
    if U[i,i]==0.0:
      if b[i]!= 0.0:
        print("System has no solution.")
    else:
      for j in range(i,n):
        b_copy[i] -=U[i,j]*x[j]
      x[i]= b_copy[i]/U[i,i]
  return x

# Add ones
def add_ones(X):

  # ADD YOUR CODES
  a=np.ones((X.shape[0],1))
  b=X[:,:]
  X_new=np.hstack((a,b))
  return X_new

def split_data(X,Y, train_size):
  # ADD YOUR CODES
  # shuffle the data before splitting it
  Y=Y.reshape((Y.shape[0],1))
  #print(Y.shape)
  data=np.hstack((X,Y))
  
  data1=np.take(data,np.random.permutation(data.shape[0]),axis=0,out=data)

  size=int(train_size*X.shape[0])
  #print(np.shape(data1))
  X_train=data1[:size,:-1]
  X_test=data1[size:,:-1]
  Y_train=data1[:size,-1]
  Y_test=data1[size:, -1]

  return X_train, X_test, Y_train.reshape(Y_train.shape[0],1), Y_test.reshape(Y_test.shape[0],1)

def mse(y, y_pred):
    mean_sq_error = (1/len(y)) * np.sum((y-y_pred)**2)
    return mean_sq_error

def normalEquation(X,y):
    theta = np.dot(np.linalg.inv(X.T@X),np.dot(X.T,y))
    return theta

class LinearRegression:

  def __init__(self,arg):
    self.theta1 = 0
    self.arg = arg
      
  def fit(self,x,y):
    if self.arg == "Normal Equation":
      self.theta = normalEquation(x,y)
    elif self.arg == "Gram Schmidt":
      Q1, R1 = cgs(x)
      self.theta  =backsubs(R1, np.dot(Q1.T,y))
    
  def predict(self,x):
    predicted_values = x@self.theta
    return predicted_values

# getting the dataset
X_data, Y_data = get_data()

# adding a new column to the features with 1's
new_X_data = add_ones(X_data)

# splitting the dataset into train and test sets
X_train, X_test, y_train, y_test = split_data(new_X_data, Y_data, 0.8)

# Instantiate the LinearRegression class 
# You can choose between the Normal Equation method or the Gram Schmidt Method
# For the Normal Equation, type "Normal Method" in the argument of the class, and "Gram Schmidt" for the Gram Schmidt method.
model2= LinearRegression("Normal Equation")

# Train the model
model2.fit(X_train,y_train)

# Make a prediction on X_test
y_pred3 = model2.predict(X_test)

# Compute the MSE (Evaluate both, regression and classification)
mse(y_test, y_pred3)